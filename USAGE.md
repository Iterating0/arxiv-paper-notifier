# 使用说明

## 工作流程

程序现在的逻辑是：

1. **爬取阶段**：从arXiv爬取50篇论文（CRAWL_RESULTS）
2. **去重阶段**：从50篇中过滤出未发送过的新论文
3. **发送阶段**：只发送前10篇新论文（MAX_PAPERS_PER_DAY）
4. **记录阶段**：将发送的论文ID保存到数据库

这样确保每天最多发送10篇新论文，即使爬取到更多新论文。

## 配置参数

### MAX_PAPERS_PER_DAY（默认：10）
- 每日最多发送的新论文数量
- 建议值：5-20篇
- 太多会导致邮件过长，太少可能错过重要论文

### CRAWL_RESULTS（默认：50）
- 每次爬取的论文总数（用于筛选）
- 建议值：30-100篇
- 太少可能筛选不到足够的新论文
- 太多会增加API请求时间

### CRAWL_INTERVAL_DAYS（默认：1）
- 查询最近几天的论文
- 建议值：1-7天
- 1天：只看最新论文
- 7天：包含本周所有论文

## 使用示例

### 每日发送10篇新论文
```bash
python main.py -t "machine learning" --daily 09:00
```

### 每日发送5篇新论文（修改配置）
在`.env`文件中：
```
MAX_PAPERS_PER_DAY=5
```

### 自定义爬取范围
```bash
# 爬取最近3天的论文
# 在代码中修改 CRAWL_INTERVAL_DAYS=3
```

## 场景说明

### 场景1：新论文很多
- 爬取50篇 → 找到30篇新论文 → 只发送前10篇
- 剩余20篇新论文会保留，下次再发送

### 场景2：新论文很少
- 爬取50篇 → 找到3篇新论文 → 只发送3篇
- 不会重复发送已发送过的论文

### 场景3：没有新论文
- 爬取50篇 → 找到0篇新论文 → 不发送邮件
- 程序继续运行，等待下次检查
